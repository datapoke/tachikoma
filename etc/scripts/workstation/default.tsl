v2
include services/config.tsl

make_node CommandInterpreter hosts
make_node JobController      jobs
make_node Ruleset            server_log:ruleset
make_node Tee                server_log:tee
make_node LogColor           server_log:color
make_node Tee                server_log
make_node Tee                error_log:tee
make_node LogColor           error_log:color
make_node Tee                error_log
make_node Ruleset            local_system_log:ruleset
make_node Ruleset            system_log:ruleset
make_node Tee                system_log:tee
make_node LogColor           system_log:color
make_node Tee                system_log
make_node Tee                silc_dn:tee
make_node Tee                http_log
make_node Null               null
make_node Echo               echo
make_node Scheduler          scheduler

cd server_log:ruleset:config
  add  100 cancel where payload=.* FROM: .* ID: "tachikoma@<hostname>(?:\.?.*)" COMMAND: .*
  add  200 cancel where payload=silo .* pub .* user .* addr .* is rfc1918
  add  999 copy to error_log:tee where payload="ERROR:|FAILED:|TRAP:|COMMAND:"
  add 1000 redirect to server_log:tee
cd ..

cd local_system_log:ruleset:config
  add 100 allow where payload=sudo
  add 1000 cancel
cd ..

cd system_log:ruleset:config
  add 200  deny where payload=ipmi0: KCS
  add 1000 redirect to system_log:tee
cd ..

command jobs start_job TailForks tails localhost:4230
cd tails
  add_tail /var/log/tachikoma/tachikoma-server.log server_log:ruleset
  add_tail /var/log/tachikoma/http-access.log      http_log
  add_tail /var/log/tachikoma/tasks-access.log     http_log
  add_tail /var/log/tachikoma/tables-access.log    http_log
cd ..

connect_node system_log:color         system_log
connect_node system_log:tee           system_log:color
connect_node local_system_log:ruleset system_log:ruleset
connect_node error_log:color          error_log
connect_node error_log:tee            error_log:color
connect_node server_log:color         server_log
connect_node server_log:tee           server_log:color


func run_benchmarks {
    command jobs start_job CommandInterpreter benchmarks;

    cd benchmarks;
      listen_inet       127.0.0.1:5000;
      listen_inet       127.0.0.1:5001;
      listen_inet       127.0.0.1:5002;
      listen_inet --io  127.0.0.1:6000;
      listen_inet --io  127.0.0.1:6001;
      listen_inet --io  127.0.0.1:6002;
      make_node Null null;
      connect_edge 127.0.0.1:5000 null;
      connect_edge 127.0.0.1:6000 null;

      on 127.0.0.1:5001 AUTHENTICATED {
          make_node Null <1>:timer 0 512 100;
          connect_sink <1>:timer <1>;
      };
      on 127.0.0.1:5001 EOF rm <1>:timer;

      on 127.0.0.1:5002 AUTHENTICATED {
          make_node Null <1>:timer 0 16 65000;
          connect_sink <1>:timer <1>;
      };
      on 127.0.0.1:5002 EOF rm <1>:timer;

      on 127.0.0.1:6001 CONNECTED {
          make_node Null <1>:timer 0 512 100;
          connect_sink <1>:timer <1>;
      };
      on 127.0.0.1:6001 EOF rm <1>:timer;

      on 127.0.0.1:6002 CONNECTED {
          make_node Null <1>:timer 0 16 65000;
          connect_sink <1>:timer <1>;
      };
      on 127.0.0.1:6002 EOF rm <1>:timer;
      insecure;
    cd ..;
}

func run_benchmarks_profiled {
    local time = <1>;
    env NYTPROF=addpid=1:file=/tmp/nytprof.out;
    env PERL5OPT=-d:NYTProf;
    run_benchmarks;
    if (<time>) {
        command scheduler in <time> command jobs stop_job benchmarks;
    };
    env NYTPROF=;
    env PERL5OPT=;
}



# partitions
command jobs start_job CommandInterpreter partitions
cd partitions
  make_node Partition scratch:log     --filename=/logs/scratch.log         \
                                      --segment_size=(32 * 1024 * 1024)
  make_node Partition follower:log    --filename=/logs/follower.log        \
                                      --segment_size=(32 * 1024 * 1024)    \
                                      --leader=scratch:log
  make_node Partition offset:log      --filename=/logs/offset.log          \
                                      --segment_size=(256 * 1024)
  make_node Consumer scratch:consumer --partition=scratch:log              \
                                      --offsetlog=offset:log
  insecure
cd ..



# services
var services = "<home>/.tachikoma/services";
func start_service {
    local service = <1>;
    command jobs  start_job Shell <service>:job <services>/<service>.tsl;
    command hosts connect_inet localhost:[var "tachikoma.<service>.port"] <service>:service;
    command tails add_tail /var/log/tachikoma/<service>.log server_log:ruleset;
}
func stop_service {
    local service = <1>;
    command jobs stop_job <service>:job;
    # rm <service>:service;
}
for service (<tachikoma.services>) {
    start_service <service>;
}



# ingest server logs
connect_node server_log:tee server_logs:service/server_log

var username=`whoami`;

# sound effects
func get_sound   { return "/System/Library/Sounds/<1>.aiff\n" }
func afplay      { send AfPlay:sieve <1>; return }

make_node MemorySieve AfPlay:sieve     1
make_node JobFarmer   AfPlay           4 AfPlay
make_node Function    server_log:sounds '{
    local sound = "";
    # if (<1> =~ "\sWARNING:\s")    [ sound = Tink;   ]
    if (<1> =~ "\sERROR:\s(.*)")  [ sound = Tink;   ]
    elsif (<1> =~ "\sFAILURE:\s") [ sound = Sosumi; ]
    elsif (<1> =~ "\sCOMMAND:\s") [ sound = Hero;   ];
    if (<sound>) { afplay { get_sound <sound> } };
}'
make_node Function silc:sounds '{
    local sound = Pop;
    if (<1> =~ "\b<username>\b(?!>)") [ sound = Glass ];
    afplay { get_sound <sound> };
}'
command AfPlay lazy on
connect_node AfPlay:sieve      AfPlay:load_balancer
connect_node server_log:sounds _responder
connect_node server_log:tee    server_log:sounds
connect_node silc:sounds       _responder
connect_node silc_dn:tee       silc:sounds

insecure
